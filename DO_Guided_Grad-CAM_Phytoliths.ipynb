{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNBU/1lRbXzeQouD+oqdaJH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Deep Learning Black-Box and Pattern Recognition Analysis Using Guided Grad-CAM for Phytolith Identification\n","\n","Iban Berganzo-Besga 1,2,*, Hector A. Orengo 3,4,2, Felipe Lumbreras 5,6, Monica N. Ramsey 1\n","\n","1. Ramsey Laboratory for Environmental Archaeology (RLEA), University of Toronto Mississauga (UTM), 3359 Mississauga Road, Mississauga, ON, L5L 1C6, Canada\n","\n","2. Landscape Archaeology Research Group (GIAP), Catalan Institute of Classical Archaeology (ICAC), Plaça Rovellat s/n, 43003, Tarragona, Spain\n","\n","3. Catalan Institution for Research and Advanced Studies (ICREA), Passeig Lluís Companys 23, 08010, Barcelona, Spain\n","\n","4. Computational Social Sciences and Humanities Department, Barcelona Supercomputing Center (BSC-CNS), Plaça d'Eusebi Güell, 1-3, Les Corts, 08034, Barcelona, Spain\n","\n","5. Computer Vision Center (CVC), 08193, Bellaterra (Cerdanyola del Vallès), Spain\n","\n","6. Department of Computer Science, Universitat Autònoma de Barcelona (UAB), 08193, Bellaterra (Cerdanyola del Vallès), Spain\n","\n","\\* Correspondence author\n","\n","\n","---"],"metadata":{"id":"fWn5hnJ_0g0z"}},{"cell_type":"markdown","source":["# VGG19 and TensorFlow 2 Guided Grad-CAM Implementation"],"metadata":{"id":"9eQ5Ol9q8420"}},{"cell_type":"markdown","source":["# 1. Initialization\n","\n","---"],"metadata":{"id":"_l3axJRg1MdJ"}},{"cell_type":"code","source":["# Set TensorFlow version to 2.15\n","\n","# !pip install tf-keras==2.15.0 tensorflow==2.15\n","!pip install tf-keras==2.15.0 tensorflow==2.15 keras==2.15.0 ml-dtypes==0.2.0 tensorboard==2.15.2 tensorflow-estimator==2.15.0 wrapt==1.14.1 --no-deps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPUZ8VC-jeIw","executionInfo":{"status":"ok","timestamp":1730355732165,"user_tz":-60,"elapsed":67790,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}},"outputId":"995d4c37-5383-4a0b-e4b8-333f6bc31920"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-keras==2.15.0\n","  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting tensorflow==2.15\n","  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Collecting keras==2.15.0\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting ml-dtypes==0.2.0\n","  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting tensorboard==2.15.2\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator==2.15.0\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting wrapt==1.14.1\n","  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tf-keras, tensorflow-estimator, tensorflow, tensorboard, ml-dtypes, keras\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.16.0\n","    Uninstalling wrapt-1.16.0:\n","      Successfully uninstalled wrapt-1.16.0\n","  Attempting uninstall: tf-keras\n","    Found existing installation: tf_keras 2.17.0\n","    Uninstalling tf_keras-2.17.0:\n","      Successfully uninstalled tf_keras-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.1\n","    Uninstalling ml-dtypes-0.4.1:\n","      Successfully uninstalled ml-dtypes-0.4.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","Successfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tf-keras-2.15.0 wrapt-1.14.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSa58TP66qOi","executionInfo":{"status":"ok","timestamp":1730355749603,"user_tz":-60,"elapsed":10882,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}},"outputId":"e9e2861c-b148-4dcf-8fa6-be531488e54e"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version:  2.15.0\n"]}],"source":["# Imports\n","\n","import os\n","import cv2\n","import glob\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from skimage import io\n","from tensorflow.python.framework import ops\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n","\n","print(\"TensorFlow version: \", tf.__version__)"]},{"cell_type":"code","source":["# GitHub Repository\n","\n","!git clone https://github.com/iberganzo/ArchaeolGradCAM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhdVMmJsHBxh","executionInfo":{"status":"ok","timestamp":1730355762160,"user_tz":-60,"elapsed":3018,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}},"outputId":"e5eece44-a06f-4bef-8842-0074b064b4a1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ArchaeolGradCAM'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Counting objects: 100% (56/56), done.\u001b[K\n","remote: Compressing objects: 100% (50/50), done.\u001b[K\n","remote: Total 56 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (56/56), 8.47 MiB | 15.52 MiB/s, done.\n","Resolving deltas: 100% (6/6), done.\n"]}]},{"cell_type":"code","source":["%cd ArchaeolGradCAM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfu35xdmNPWp","executionInfo":{"status":"ok","timestamp":1730355765959,"user_tz":-60,"elapsed":774,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}},"outputId":"23e1569b-7d63-46cb-90c4-35d30a191b12"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ArchaeolGradCAM\n"]}]},{"cell_type":"code","source":["# Paths\n","\n","os.mkdir('Tiles')\n","os.mkdir('GradCAM')\n","os.mkdir('Guided_Backpropagation')\n","os.mkdir('Guided_GradCAM')\n","\n","path=os.path.abspath(os.getcwd())\n","imagesPath = os.path.join(path,'DatatoAnalyse')\n","tilesPath = os.path.join(path,'Tiles')\n","GCPath = os.path.join(path,'GradCAM')\n","guidedBPPath = os.path.join(path,'Guided_Backpropagation')\n","guidedGCPath = os.path.join(path,'Guided_GradCAM')"],"metadata":{"id":"Ke2PiDUjZGOl","executionInfo":{"status":"ok","timestamp":1730355769644,"user_tz":-60,"elapsed":2,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Download the weights of the trained model\n","\n","!apt-get update\n","!apt-get install megatools\n","\n","!megadl 'https://mega.nz/#!ER8GXIgS!--jyrQr_ImHiGhpqIckF2F4q55e1M2dKss4nU2t9JJE'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAH8yzsO7DQX","executionInfo":{"status":"ok","timestamp":1730355799445,"user_tz":-60,"elapsed":27424,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}},"outputId":"a7930dbe-c67f-4fb5-afe2-3129e2ece74c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,091 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Ign:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,227 kB]\n","Get:11 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n","Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,606 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,305 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,162 kB]\n","Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,389 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,665 kB]\n","Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,432 kB]\n","Fetched 26.7 MB in 5s (5,509 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  megatools\n","0 upgraded, 1 newly installed, 0 to remove and 51 not upgraded.\n","Need to get 207 kB of archives.\n","After this operation, 898 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 megatools amd64 1.10.3-1build1 [207 kB]\n","Fetched 207 kB in 1s (159 kB/s)\n","Selecting previously unselected package megatools.\n","(Reading database ... 123623 files and directories currently installed.)\n","Preparing to unpack .../megatools_1.10.3-1build1_amd64.deb ...\n","Unpacking megatools (1.10.3-1build1) ...\n","Setting up megatools (1.10.3-1build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","\u001b[0KDownloaded RGB_RJBDA_VGG19_PreTrained_Ref_5C_1.h5\n"]}]},{"cell_type":"code","source":["# Auxiliary functions #\n","\n","def generate_prediction (img):\n","\n","    imageR = img.resize((sx,sy))\n","    image_array = np.array(imageR)\n","    image_array = image_array.reshape((sy,sx,ch))\n","    pred_images = np.zeros(shape=(1,sy,sx,ch))\n","    pred_images[0] = image_array\n","    pred_images = pred_images.astype('uint8')\n","\n","    pred = model.predict(pred_images, batch_size=1)\n","\n","    pred_index = np.argmax(pred)\n","\n","    print(\"Prediction: \", class_names[pred_index])\n","\n","    return pred_index"],"metadata":{"id":"2TZ_IG9D_HUf","executionInfo":{"status":"ok","timestamp":1730355807358,"user_tz":-60,"elapsed":846,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def load_image(path, preprocess=True):\n","    \"\"\"Load and preprocess image.\"\"\"\n","    x = image.load_img(path, target_size=(sy, sx))\n","    if preprocess:\n","        x = image.img_to_array(x)\n","        x = np.expand_dims(x, axis=0)\n","        x = preprocess_input(x)\n","    return x\n","\n","def deprocess_image(x):\n","    \"\"\"Same normalization as in:\n","    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n","    \"\"\"\n","    # normalize tensor: center on 0., ensure std is 0.25\n","    x = x.copy()\n","    x -= x.mean()\n","    x /= (x.std() + K.epsilon())\n","    x *= 0.25\n","\n","    # clip to [0, 1]\n","    x += 0.5\n","    x = np.clip(x, 0, 1)\n","\n","    # convert to RGB array\n","    x *= 255\n","    if K.image_data_format() == 'channels_first':\n","        x = x.transpose((1, 2, 0))\n","    x = np.clip(x, 0, 255).astype('uint8')\n","    return x"],"metadata":{"id":"vV8DZI9L884c","executionInfo":{"status":"ok","timestamp":1730355813887,"user_tz":-60,"elapsed":635,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Gradient-weighted Class Activation Mapping (Grad-CAM) #\n","\n","def generate_grad_cam(img):\n","    img = cv2.resize(img, (sx, sy))\n","    img = img.astype(np.float32) / 255.0\n","    img = np.expand_dims(img, axis=0)\n","\n","    # Gradients\n","    with tf.GradientTape() as tape:\n","        conv_outputs, predictions = grad_model(img)\n","        loss = predictions[:, target_class_index]\n","\n","    grads = tape.gradient(loss, conv_outputs)[0]\n","    grads = tf.reduce_mean(grads, axis=(0, 1))\n","\n","    cam = tf.reduce_sum(tf.multiply(conv_outputs[0], grads), axis=-1)\n","\n","    # Normalise\n","    cam = cam.numpy()\n","    cam = cv2.resize(cam, (sy, sx))\n","    cam = np.maximum(cam, 0)\n","    cam = cam / cam.max()\n","\n","    # Heatmap\n","    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n","    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n","    heatmap = np.float32(heatmap) / 255\n","\n","    # Combine guided_bp with the original image\n","    img_gc = cv2.imread(image_path)\n","    img_gc = cv2.resize(img_gc, (sx, sy))\n","    img_gc = img_gc.astype(np.float32) / 255.0\n","    img_gc = np.expand_dims(img_gc, axis=0)\n","    img_gc = img_gc[0]\n","    img_gc = np.float32(cv2.cvtColor(img_gc, cv2.COLOR_RGB2BGR))\n","    overlay_gc = heatmap + img_gc\n","    overlay_gc = overlay_gc / np.max(overlay_gc)\n","\n","    gradcam =  np.uint8(255 * overlay_gc)\n","\n","    # print(\"Gradcam shape:\", gradcam.shape)\n","\n","    return cam, heatmap, gradcam"],"metadata":{"id":"o5jQRec4_ti_","executionInfo":{"status":"ok","timestamp":1730355822075,"user_tz":-60,"elapsed":507,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Guided Backpropagation #\n","\n","@tf.custom_gradient\n","def guidedRelu(x):\n","  def grad(dy):\n","    return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n","  return tf.nn.relu(x), grad"],"metadata":{"id":"d3jw19B--7nO","executionInfo":{"status":"ok","timestamp":1730355825967,"user_tz":-60,"elapsed":617,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def generate_guided_backpropagation(preprocessed_input):\n","    layer_dict = [layer for layer in gb_model.layers[1:] if hasattr(layer,'activation')]\n","    for layer in layer_dict:\n","      if layer.activation == tf.keras.activations.relu:\n","        layer.activation = guidedRelu\n","\n","    # Gradients\n","    with tf.GradientTape() as tape:\n","      inputs = tf.cast(preprocessed_input, tf.float32)\n","      tape.watch(inputs)\n","      outputs = gb_model(inputs)\n","\n","    grads = tape.gradient(outputs,inputs)[0]\n","\n","    guided_bp1 = np.flip(deprocess_image(np.array(grads)),-1)\n","\n","    gradBGR =  grads.numpy()\n","\n","    guided_bp = np.dstack((\n","        gradBGR[:, :, 2],  # R\n","        gradBGR[:, :, 1],  # G\n","        gradBGR[:, :, 0],  # B\n","    ))\n","\n","    # Combine guided_bp with the original image\n","    img_bp = cv2.imread(image_path)\n","    img_bp = cv2.resize(img_bp, (sx, sy))\n","    img_bp = img_bp.astype(np.float32) / 255.0\n","    img_bp = np.expand_dims(img_bp, axis=0)\n","    img_bp = img_bp[0]\n","    img_bp = np.float32(cv2.cvtColor(img_bp, cv2.COLOR_RGB2BGR))\n","    overlay_bp = img_bp + guided_bp1\n","    overlay_bp = overlay_bp / np.max(overlay_bp)\n","\n","    guided_backprop =  np.uint8(255 * overlay_bp)\n","\n","    # print(\"Guided_bp shape:\", guided_bp.shape)\n","\n","    return guided_bp, guided_bp1, guided_backprop"],"metadata":{"id":"HSvP2Y6u-8AJ","executionInfo":{"status":"ok","timestamp":1730355828220,"user_tz":-60,"elapsed":2,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Guided Grad-CAM #\n","\n","def generate_guided_grad_cam(cam, guided_bp):\n","\n","  guided_gradcam = np.dstack((\n","        guided_bp[:, :, 0] * cam,\n","        guided_bp[:, :, 1] * cam,\n","        guided_bp[:, :, 2] * cam,\n","    ))\n","\n","  guided_gradcam -= np.min(guided_gradcam)\n","  guided_gradcam /= guided_gradcam.max()\n","\n","  # print(\"Guided_gradcam shape:\", guided_gradcam.shape)\n","\n","  return guided_gradcam"],"metadata":{"id":"8ZqvwvhE_aOO","executionInfo":{"status":"ok","timestamp":1730355832925,"user_tz":-60,"elapsed":703,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Auxiliary functions #\n","\n","def visualize_guided_backpropagation(guided_bp):\n","\n","  guided_bp_v = np.dstack((\n","        guided_bp[:, :, 0],\n","        guided_bp[:, :, 1],\n","        guided_bp[:, :, 2],\n","    ))\n","\n","  guided_bp_v -= np.min(guided_bp_v)\n","  guided_bp_v /= guided_bp_v.max()\n","\n","  return guided_bp_v"],"metadata":{"id":"HYwIPE1k0MxQ","executionInfo":{"status":"ok","timestamp":1730355835998,"user_tz":-60,"elapsed":608,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def extract_number_after_split(filename):\n","    parts = filename.split('_split_')\n","    if len(parts) > 1:\n","        return int(''.join(filter(str.isdigit, parts[1])))\n","    else:\n","        return float('inf')"],"metadata":{"id":"DoA7mZW5svk5","executionInfo":{"status":"ok","timestamp":1730355837648,"user_tz":-60,"elapsed":496,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def merge_images(target_layer, typeName, imgOriginal, input_folder, imName, width, height, channels):\n","    # Merge all the cropped images\n","    merged_image = Image.new('RGBA', (width, height)) # Empty image\n","    gc_splitted_images = sorted(os.listdir(input_folder), key=extract_number_after_split)\n","\n","    x, y = 0, 0\n","    for gc_splitted_image in gc_splitted_images:\n","        if gc_splitted_image.endswith(('.jpg', '.png')):\n","            gc_image = Image.open(os.path.join(input_folder, gc_splitted_image))\n","            merged_image.paste(gc_image, (x, y))\n","            x += gc_image.width\n","            if x >= width:\n","                x = 0\n","                y += gc_image.height\n","\n","    merged_image.save('%s_%s_%s.png' %(imName, target_layer, typeName))\n","\n","    # Have the original image as 0.5 gray background\n","    aux_img = imgOriginal.convert('L')\n","    aux_img = aux_img.convert('RGBA')\n","    if typeName == 'GradCAM':\n","        blended_image = Image.blend(aux_img, merged_image, alpha=0.5)\n","    else:\n","        blended_image = Image.alpha_composite(aux_img, merged_image)\n","    blended_image.save('%s_%s_%s_back.png' %(imName, target_layer, typeName))\n","\n","    print(\"%s created: %s \" %(typeName,imName))\n","\n","    return blended_image"],"metadata":{"id":"lNbPjYy7fmWH","executionInfo":{"status":"ok","timestamp":1730355839690,"user_tz":-60,"elapsed":610,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# 2. Processing\n","\n","---"],"metadata":{"id":"dFBW4FR91k-T"}},{"cell_type":"code","source":["# VGG19 #\n","\n","# Define the parameters\n","\n","tesela = 256 # Image size\n","channels = 3 # Image channels\n","target_layer_list = [\"block5_pool\"] # Network layer\n","\n","# First it learns from basic elements such as edges and then from combinations of the elements learnt in previous layers\n","#target_layer_list = [ # VGG19 layers\n","#    \"block1_conv1\", \"block1_conv2\", \"block1_pool\", \"block2_conv1\", \"block2_conv2\",\n","#    \"block2_pool\", \"block3_conv1\", \"block3_conv2\", \"block3_conv3\", \"block3_conv4\",\n","#    \"block3_pool\", \"block4_conv1\", \"block4_conv2\", \"block4_conv3\", \"block4_conv4\",\n","#    \"block4_pool\",\"block5_conv1\", \"block5_conv2\", \"block5_conv3\", \"block5_conv4\", \"block5_pool\"]\n","\n","numclasses = 5 # Model number of classes\n","class_names = ['Avena','Hordeum','Triticum', 'Background', 'Artifact'] # Model classes' names\n","\n","target_class_index = 2 # Model class to analyse using Guided Grad-CAM: 0: Avena, 1: Hordeum, 2: Triticum, 3: Background, 4: Artifact\n","cam_th = 0.75 # Guided Grad-CAM threshold based on Grad-CAM heatmap"],"metadata":{"id":"CXxvg0v3KTOl","executionInfo":{"status":"ok","timestamp":1730355843176,"user_tz":-60,"elapsed":534,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","\n","sx, sy, ch = tesela, tesela, channels\n","base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(sx, sy, ch))\n","model = tf.keras.Sequential([\n","    base_model,\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(numclasses, activation=\"softmax\")\n","])\n","model.load_weights('RGB_RJBDA_VGG19_PreTrained_Ref_5C_1.h5')\n","\n","print(\"MODEL: \")\n","# Print model layers\n","for layer in model.layers:\n","    print('Layer: ', layer.name)\n","\n","print(\"\\n\")\n","print(\"VGG19: \")\n","# Print base model layers\n","for layer in base_model.layers:\n","    print('Layer: ', layer.name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mH9XPrYKKYap","executionInfo":{"status":"ok","timestamp":1730355851306,"user_tz":-60,"elapsed":6502,"user":{"displayName":"e2 archaeologist","userId":"06487699756254162662"}},"outputId":"bbdefd36-2b8c-405b-baf2-f60a24f10143"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80134624/80134624 [==============================] - 4s 0us/step\n","MODEL: \n","Layer:  vgg19\n","Layer:  global_average_pooling2d\n","Layer:  dense\n","\n","\n","VGG19: \n","Layer:  input_1\n","Layer:  block1_conv1\n","Layer:  block1_conv2\n","Layer:  block1_pool\n","Layer:  block2_conv1\n","Layer:  block2_conv2\n","Layer:  block2_pool\n","Layer:  block3_conv1\n","Layer:  block3_conv2\n","Layer:  block3_conv3\n","Layer:  block3_conv4\n","Layer:  block3_pool\n","Layer:  block4_conv1\n","Layer:  block4_conv2\n","Layer:  block4_conv3\n","Layer:  block4_conv4\n","Layer:  block4_pool\n","Layer:  block5_conv1\n","Layer:  block5_conv2\n","Layer:  block5_conv3\n","Layer:  block5_conv4\n","Layer:  block5_pool\n"]}]},{"cell_type":"code","source":["# Main #\n","\n","for itL in range(0,len(target_layer_list),1):\n","\n","  target_layer = target_layer_list[itL]\n","  print(\"\\n\")\n","  print(\"Target Layer: \", target_layer)\n","\n","  input_tensor = model.layers[0].input\n","  inputs_tensor = model.layers[0].inputs\n","  last_conv_layer = model.layers[0].get_layer(target_layer)\n","  grad_model = tf.keras.models.Model(inputs=input_tensor, outputs=[last_conv_layer.output, model.layers[0].output]) # Grad-CAM\n","  gb_model = Model(inputs = [inputs_tensor], outputs = [last_conv_layer.output]) # Guided Backpropagation\n","\n","  for filenameR in glob.glob(\"DatatoAnalyse/.*\"): # Remove previous files\n","      if os.path.isdir(filenameR):\n","            os.rmdir(filenameR)\n","      else:\n","            os.remove(filenameR)\n","\n","  images_list=os.listdir(imagesPath)\n","  images_list=sorted(images_list)\n","  for iM in range(0,len(images_list),1):\n","\n","      print(\"\\n\")\n","      print(\"Data to Analyse: \", images_list[iM])\n","\n","      # Splitting the image into tiles\n","      for filenameR in glob.glob(\"Tiles/*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"Tiles/.*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"GradCAM/*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"GradCAM/.*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"Guided_Backpropagation/*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"Guided_Backpropagation/.*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"Guided_GradCAM/*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","      for filenameR in glob.glob(\"Guided_GradCAM/.*\"): # Remove previous files\n","            if os.path.isdir(filenameR):\n","                  os.rmdir(filenameR)\n","            else:\n","                  os.remove(filenameR)\n","\n","      imgOriginal = cv2.imread('DatatoAnalyse/%s' %images_list[iM])\n","      imgOriginal_pil = Image.open('DatatoAnalyse/%s' %images_list[iM])\n","      height, width, channels = imgOriginal.shape\n","      [imName,imExt] = os.path.splitext(\"%s\" %images_list[iM])\n","\n","      num=1\n","      for i in range(0,height,tesela):\n","        for j in range(0,width,tesela):\n","          cropped_image = imgOriginal[i:i+tesela, j:j+tesela]\n","          tilesPath1 = os.path.join(tilesPath,'%s_split_%d.jpg' %(imName,num))\n","          cv2.imwrite(tilesPath1, cropped_image)\n","          num=num+1\n","\n","      tiles_list=os.listdir(tilesPath)\n","      tiles_list = sorted(tiles_list, key=extract_number_after_split)\n","      for iM2 in range(0,len(tiles_list),1):\n","\n","          print(\"Analysed: \", tiles_list[iM2])\n","\n","          image_path = os.path.join(tilesPath, tiles_list[iM2])\n","          preprocessed_input = load_image(image_path)\n","          img_original = cv2.imread(image_path)\n","          img_rgb = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n","          img_pil = Image.open(image_path)\n","\n","          pred_index = generate_prediction(img_pil)\n","          image_tr = Image.new('RGBA', (tesela, tesela), (0, 0, 0, 0)) # Transparent image\n","\n","          if pred_index == target_class_index:\n","              cam, heatmap, gradcam = generate_grad_cam(img_original)\n","              cam1 = cam; cam1[cam1 < cam_th] = 0\n","              guided_bp, guided_bp1, guided_backprop = generate_guided_backpropagation(preprocessed_input)\n","              guided_bp_v = visualize_guided_backpropagation(guided_bp)\n","              guided_gc = generate_guided_grad_cam(cam1, guided_bp)\n","\n","              output_path = os.path.join(GCPath, '%s_split_%d.jpg' %(imName,iM2))\n","              plt.imsave(output_path, heatmap)\n","              output_path = os.path.join(guidedBPPath, '%s_split_%d.jpg' %(imName,iM2))\n","              plt.imsave(output_path, guided_bp_v)\n","              output_path = os.path.join(guidedGCPath, '%s_split_%d.jpg' %(imName,iM2))\n","              plt.imsave(output_path, guided_gc)\n","          else:\n","              output_path = os.path.join(GCPath, '%s_split_%d.png' %(imName,iM2))\n","              image_tr.save(output_path)\n","              output_path = os.path.join(guidedBPPath, '%s_split_%d.png' %(imName,iM2))\n","              image_tr.save(output_path)\n","              output_path = os.path.join(guidedGCPath, '%s_split_%d.png' %(imName,iM2))\n","              image_tr.save(output_path)\n","\n","      # Merge the Guided Grad-CAM divided images\n","\n","      merged_image_gc = merge_images(target_layer, 'GradCAM', imgOriginal_pil, GCPath, imName, width, height, channels)\n","      merged_image_gbp = merge_images(target_layer, 'GuidedBackProp', imgOriginal_pil, guidedBPPath, imName, width, height, channels)\n","      merged_image_ggc = merge_images(target_layer, 'GuidedGradCAM', imgOriginal_pil, guidedGCPath, imName, width, height, channels)\n","\n","      plt.figure()\n","      plt.imshow(merged_image_gc)\n","      plt.title('Grad-CAM %s %s' %(images_list[iM], target_layer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPz_LAakAW8T","outputId":"710afb32-a5d7-4d61-af77-3464b01b5ac8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Target Layer:  block5_pool\n","\n","\n","Data to Analyse:  Figure1.jpg\n","Analysed:  Figure1_split_1.jpg\n","1/1 [==============================] - 2s 2s/step\n","Prediction:  Background\n","Analysed:  Figure1_split_2.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_3.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_4.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_5.jpg\n","1/1 [==============================] - 0s 22ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_6.jpg\n","1/1 [==============================] - 0s 25ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_7.jpg\n","1/1 [==============================] - 0s 29ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_8.jpg\n","1/1 [==============================] - 0s 33ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_9.jpg\n","1/1 [==============================] - 0s 25ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_10.jpg\n","1/1 [==============================] - 0s 27ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_11.jpg\n","1/1 [==============================] - 0s 31ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_12.jpg\n","1/1 [==============================] - 0s 33ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_13.jpg\n","1/1 [==============================] - 0s 28ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_14.jpg\n","1/1 [==============================] - 0s 30ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_15.jpg\n","1/1 [==============================] - 0s 32ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_16.jpg\n","1/1 [==============================] - 0s 25ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_17.jpg\n","1/1 [==============================] - 0s 24ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_18.jpg\n","1/1 [==============================] - 0s 28ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_19.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_20.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_21.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_22.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_23.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_24.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_25.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_26.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_27.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_28.jpg\n","1/1 [==============================] - 0s 19ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_29.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_30.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_31.jpg\n","1/1 [==============================] - 0s 19ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_32.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_33.jpg\n","1/1 [==============================] - 0s 20ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_34.jpg\n","1/1 [==============================] - 0s 19ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_35.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_36.jpg\n","1/1 [==============================] - 0s 21ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_37.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_38.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Triticum\n","Analysed:  Figure1_split_39.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_40.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_41.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_42.jpg\n","1/1 [==============================] - 0s 23ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_43.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_44.jpg\n","1/1 [==============================] - 0s 19ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_45.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_46.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_47.jpg\n","1/1 [==============================] - 0s 23ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_48.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_49.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_50.jpg\n","1/1 [==============================] - 0s 18ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_51.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Background\n","Analysed:  Figure1_split_52.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_53.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_54.jpg\n","1/1 [==============================] - 0s 20ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_55.jpg\n","1/1 [==============================] - 0s 30ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_56.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_57.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_58.jpg\n","1/1 [==============================] - 0s 16ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_59.jpg\n","1/1 [==============================] - 0s 29ms/step\n","Prediction:  Artifact\n","Analysed:  Figure1_split_60.jpg\n","1/1 [==============================] - 0s 17ms/step\n","Prediction:  Artifact\n","GradCAM created: Figure1 \n","GuidedBackProp created: Figure1 \n","GuidedGradCAM created: Figure1 \n"]}]},{"cell_type":"markdown","source":["# 3. References\n","\n","---"],"metadata":{"id":"0pTgk9_v1na6"}},{"cell_type":"markdown","source":["1. Berganzo-Besga, I. 2022. ArchaeolPhytoliths: Automated detection and classification of multi-cell phytoliths at genera and species-level using Deep Learning algorithm. GitHub repository. Available online: https://github.com/iberganzo/ArchaeolPhytoliths (Accessed 23 August 2024)\n","2. Gildenblat J. 2017. Grad-CAM implementation in Keras. Available online: https://github.com/jacobgil/keras-grad-cam (Accessed 23 August 2024)\n","3. Petsiuk V. 2017. keras-gradcam. Available online: https://github.com/eclique/keras-gradcam (Accessed 23 August 2024)\n","4. Nguyen H. 2020. Demo GradCAM & Guided GradCAM. Available online: https://github.com/hnguyentt/GradCAM_and_GuidedGradCAM_tf2 (Accessed 23 August 2024)\n"],"metadata":{"id":"PkgvGPAg1qZs"}}]}